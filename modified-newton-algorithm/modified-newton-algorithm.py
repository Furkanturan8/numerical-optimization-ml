#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Apr 19 15:25:59 2025

@author: furkanturan

Algoritma: DeÄŸiÅŸtirilmiÅŸ Newton AlgoritmasÄ±

AdÄ±m 1: 
----------------------------------
Bir baÅŸlangÄ±Ã§ noktasÄ± (ğ± ) ve maksimum iterasyon sayÄ±sÄ± (ğ‘ )
belirle.
SonlandÄ±rma kriterleri iÃ§in ğœ€ , ğœ€ ve ğœ€ deÄŸerlerini belirle.
ğ‘˜ â† 0

AdÄ±m 2:
----------------------------------
ğ± noktasÄ±ndaki gradyant vektÃ¶rÃ¼nÃ¼ âˆ‡ğ‘“(ğ± ) hesapla
ğ± noktasÄ±ndaki Hessian matrisini âˆ‡ ğ‘“(ğ± ) hesapla
EÄŸer Hessian matrisi pozitif-tanÄ±mlÄ± ise ilerleme yÃ¶nÃ¼ olarak ğ© =
âˆ’[âˆ‡ ğ‘“(ğ± )] âˆ‡ğ‘“(ğ± ) seÃ§.
EÄŸer Hessian matrisi pozitif-tanÄ±mlÄ± deÄŸilse o zaman uygun bir matris
ilavesi (ğ„ veya ğœ‡ğˆ) ile onu pozitif-tanÄ±mlÄ± hale getir ve ilerleme yÃ¶nÃ¼
olarak ğ© = âˆ’[âˆ‡ ğ‘“(ğ± ) + ğœ‡ğˆ] âˆ‡ğ‘“(ğ± ) veya ğ© = âˆ’[âˆ‡ ğ‘“(ğ± ) +
ğ„] âˆ‡ğ‘“(ğ± ) seÃ§.
Bir-boyutlu optimizasyon ile ğ‘“(ğ± + ğ‘  ğ© ) deÄŸerini minimum yapan
adÄ±m-aralÄ±ÄŸÄ±nÄ± (ğ‘  ) bul.
ğ± = ğ± + ğ‘  ğ© kuralÄ± ile gÃ¼ncellemeyi yap.
ğ‘˜ â† ğ‘˜ + 1

AdÄ±m 3:
----------------------------------
AÅŸaÄŸÄ±daki ÅŸartlardan herhangi biri saÄŸlanÄ±yorsa algoritmayÄ± bitir,
saÄŸlanmÄ±yorsa AdÄ±m 2'ye git.
C1: ğ‘ < ğ‘˜ maksimum iterasyona ulaÅŸÄ±ldÄ±.
C2: |âˆ†ğ‘“| = |ğ‘“(ğ± ) âˆ’ ğ‘“(ğ± )| < ğœ€ fonksiyon deÄŸiÅŸmiyor.
C3: |âˆ†ğ‘¥| = |ğ± âˆ’ ğ± | < ğœ€ deÄŸiÅŸkenler deÄŸiÅŸmiyor.
C4: â€–âˆ‡ğ‘“(ğ± )â€– < ğœ€ yerel minimuma yakÄ±nsadÄ±.

"""

import numpy as np
from numpy.linalg import norm, solve, eigvals
from scipy.optimize import line_search

def modified_newton(f, grad_f, hess_f, x0, max_iter=100, eps1=1e-6, eps2=1e-6, eps3=1e-6):
    """
    DeÄŸiÅŸtirilmiÅŸ Newton AlgoritmasÄ±
    
    Parametreler:
    ------------
    f : callable
        Minimize edilecek fonksiyon
    grad_f : callable
        Fonksiyonun gradyanÄ±
    hess_f : callable
        Fonksiyonun Hessian matrisi
    x0 : ndarray
        BaÅŸlangÄ±Ã§ noktasÄ±
    max_iter : int
        Maksimum iterasyon sayÄ±sÄ±
    eps1, eps2, eps3 : float
        SonlandÄ±rma kriterleri iÃ§in tolerans deÄŸerleri
    
    DÃ¶nÃ¼ÅŸ:
    ------
    x : ndarray
        Bulunan minimum nokta
    f_min : float
        Minimum fonksiyon deÄŸeri
    success : bool
        AlgoritmanÄ±n baÅŸarÄ±lÄ± olup olmadÄ±ÄŸÄ±
    iter_count : int
        Ä°terasyon sayÄ±sÄ±
    """
    
    x = np.array(x0, dtype=float)
    n = len(x)
    k = 0
    f_prev = f(x)
    
    print("\nDeÄŸiÅŸtirilmiÅŸ Newton AlgoritmasÄ± baÅŸlatÄ±lÄ±yor...")
    print("=" * 50)
    print(f"BaÅŸlangÄ±Ã§ noktasÄ±: {x}")
    print(f"BaÅŸlangÄ±Ã§ fonksiyon deÄŸeri: {f_prev:.10f}")
    print("=" * 50)
    
    while k < max_iter:
        print(f"\nÄ°terasyon {k+1}")
        print("-" * 30)
        
        # Gradyan ve Hessian hesapla
        grad = grad_f(x)
        hess = hess_f(x)
        
        print(f"Gradyan norm: {norm(grad):.10f}")
        
        # Gradyan normu kontrolÃ¼ (C4)
        if norm(grad) < eps3:
            print("\nGradyan normu yeterince kÃ¼Ã§Ã¼k - SonlandÄ±rÄ±lÄ±yor (C4)")
            return x, f(x), True, k
        
        # Hessian matrisinin pozitif tanÄ±mlÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        eigenvals = eigvals(hess)
        if np.all(eigenvals > 0):
            # Hessian pozitif tanÄ±mlÄ±
            p = -solve(hess, grad)
            print("Hessian pozitif tanÄ±mlÄ±")
        else:
            # Hessian'Ä± pozitif tanÄ±mlÄ± yap
            mu = abs(min(0, np.min(eigenvals))) + 0.1
            p = -solve(hess + mu * np.eye(n), grad)
            print(f"Hessian pozitif tanÄ±mlÄ± deÄŸil - DÃ¼zeltme uygulandÄ± (mu={mu:.6f})")
        
        # DoÄŸru yÃ¶nde arama
        alpha = line_search(f, grad_f, x, p)[0]
        if alpha is None:
            alpha = 0.1  # VarsayÄ±lan adÄ±m boyutu
            print("Line search baÅŸarÄ±sÄ±z - VarsayÄ±lan adÄ±m boyutu kullanÄ±ldÄ±")
        else:
            print(f"Optimum adÄ±m boyutu: {alpha:.6f}")
            
        # GÃ¼ncelleme
        x_new = x + alpha * p
        f_new = f(x_new)
        
        print(f"Yeni nokta: {x_new}")
        print(f"Yeni fonksiyon deÄŸeri: {f_new:.10f}")
        print(f"Fonksiyon deÄŸiÅŸimi: {abs(f_new - f_prev):.10f}")
        print(f"Konum deÄŸiÅŸimi: {norm(x_new - x):.10f}")
        
        # SonlandÄ±rma kriterleri kontrolÃ¼
        if abs(f_new - f_prev) < eps1:  # C2
            print("\nFonksiyon deÄŸiÅŸimi yeterince kÃ¼Ã§Ã¼k - SonlandÄ±rÄ±lÄ±yor (C2)")
            return x_new, f_new, True, k
        
        if norm(x_new - x) < eps2:  # C3
            print("\nKonum deÄŸiÅŸimi yeterince kÃ¼Ã§Ã¼k - SonlandÄ±rÄ±lÄ±yor (C3)")
            return x_new, f_new, True, k
        
        # DeÄŸerleri gÃ¼ncelle
        x = x_new
        f_prev = f_new
        k += 1
    
    # Maksimum iterasyona ulaÅŸÄ±ldÄ± (C1)
    print("\nMaksimum iterasyon sayÄ±sÄ±na ulaÅŸÄ±ldÄ± - SonlandÄ±rÄ±lÄ±yor (C1)")
    return x, f(x), False, k

# Test fonksiyonu Ã¶rneÄŸi (Rosenbrock fonksiyonu)
def rosenbrock(x):
    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2

def rosenbrock_grad(x):
    return np.array([
        -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0]),
        200 * (x[1] - x[0]**2)
    ])

def rosenbrock_hess(x):
    return np.array([
        [-400 * x[1] + 1200 * x[0]**2 + 2, -400 * x[0]],
        [-400 * x[0], 200]
    ])

# AlgoritmanÄ±n test edilmesi
if __name__ == "__main__":
    x0 = np.array([-1.0, 1.0])
    result = modified_newton(rosenbrock, rosenbrock_grad, rosenbrock_hess, x0)
    
    print("SonuÃ§:")
    print(f"Minimum nokta: {result[0]}")
    print(f"Minimum deÄŸer: {result[1]:.10f}")
    print(f"BaÅŸarÄ±lÄ±: {result[2]}")
    print(f"Ä°terasyon sayÄ±sÄ±: {result[3]}")



